# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1290r2sHzwHI6QM4mfikDoVTic5DxoZNa

# **Synthetic Transaction Fraud Detection**

Srinivas Natarajan - 18BCE0048 <br/>
Rahul - 18BCE0018 <br/>
Vrinda Chopra - 18BCE0785
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import streamlit as st
import plotly.express as px
import os
import time
import pickle

import seaborn as sns
from statsmodels.tools import categorical
import matplotlib.image as mpimg

from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
from sklearn import preprocessing
from scipy.stats import skew, boxcox

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, accuracy_score,f1_score
from sklearn.metrics import average_precision_score, recall_score,classification_report

import xgboost
from xgboost.sklearn import XGBClassifier
from xgboost import plot_importance, plot_tree
import catboost
from catboost import CatBoostClassifier

import keras

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

#########################################################   Title
st.title("Digital Transaction Fraud Detection")
#########################################################



#########################################################       Reading Data

st.markdown("""---

## **Reading Data** 
""")

df = pd.read_csv('D:\VIT\SEM-6\F2 - Machine Learning\Project\Synthetic_transactions.csv')
df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', 'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})
st.dataframe(df.head(7))

st.markdown("""#### Number of Records: 6,362,620 (6.3 million transactions)""")

print("Number of records: {:,}".format(len(df)))

st.write("Are there any Null Values?: ",df.isnull().values.any())

tmp = df.loc[(df['type'].isin(['TRANSFER', 'CASH_OUT'])),:]
tmp.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1, inplace=True)
tmp = tmp.reset_index(drop=True)
a = np.array(tmp['type'])
b = categorical(a, drop=True)
tmp['type_num'] = b.argmax(1)

#########################################################



#########################################################    Exploratory Data Analysis

st.markdown("""---

## **Exploratory Data Analysis**  
""")
st.text("")
st.markdown("""
### **i) Imbalance of the Dataset**  
""")
st.text("")

st.write("The percentage of fradulent transactions is: ", round((len(df[df["isFraud"]==1])/len(df))*100,3), "%. ", \
    len(df[df["isFraud"]==0]), " vs ", len(df[df["isFraud"]==1]), " means that the dataset is extremely imbalanced.\
         To tackle this we use one of 3 main methods: ") 
st.text("")
st.markdown(">  * **Under Sampling:**  We remove instances of the majority class until both classes have an equal \
    number of instances. But this can lead to a loss as a large amount of data is discarded")
st.text("")
st.markdown(">  * **Over Sampling:**  We use Synthetic Minority Oversampling (SMOTE) to generate synthetic examples \
    of the minority class to match the size of the majority class")
st.text("")
st.markdown(">  * **Decision Tree:** Models using decision trees are less prone to generalizations than standard model \
    like Logistic Regression and SVM. This means that ensemble learing techniques are intrinsically good at handling \
        imbalances. ")
st.text("")

# Count number of fraud and not fraud data
print(df.isFraud.value_counts())
fig = plt.figure(figsize=(8,6))
ax = sns.countplot(data=df, x='isFraud')
ax.set_title("Number of Legitamate Vs Fradulent Transactions",fontsize=16)
ax.set_xlabel("Is it fradulent?",fontsize=14)
ax.set_ylabel('Number of Transactions',fontsize=14)

for p in ax.patches:
    ax.annotate( str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01), fontsize=12 )
st.pyplot(fig)

st.text("")
st.write("We can see the imbalance below where '0' indicates legitimate transactions and '1' indicates \
    fraudulent transactions. We also see that the fradulent transactions are dependent on 2 attributes: ")
st.write("  1. TRANSFER")
st.write("  2. CASH_OUT")
st.text("")


fig = plt.figure(figsize=(12,8))
ax = df.groupby(['type', 'isFraud']).size().plot(kind='bar')
ax.set_title("Number of transaction per transaction type",fontsize=16)
ax.set_xlabel("(Type, isFraud)",fontsize=14)
ax.set_ylabel("Count of transaction",fontsize=14)
for p in ax.patches:
    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01), fontsize=12)
st.pyplot(fig)

#########################################################



#########################################################  Selelcting transaction types
st.markdown("""---
### **ii) Transactions to select**
""")
st.text("")

#Groupby type   
st.write(df.groupby('type')['isFraud','isFlaggedFraud'].sum())
st.text("")
st.write('The types of fraudulent transactions are {}'.format(\
list(df.loc[df.isFraud == 1].type.drop_duplicates().values))) # only 'CASH_OUT' 

dfFraudTransfer = df.loc[(df.isFraud == 1) & (df.type == 'TRANSFER')]
dfFraudCashout = df.loc[(df.isFraud == 1) & (df.type == 'CASH_OUT')]

st.text("")
st.write('The number of fraudulent TRANSFERs = {}'.format(len(dfFraudTransfer))) # 4097
st.write('The number of fraudulent CASH_OUTs = {}'.format(len(dfFraudCashout))) # 4116
st.text("")

X = df.loc[(df.type == 'TRANSFER') | (df.type == 'CASH_OUT')]

randomState = 42
np.random.seed(randomState)

Y = X['isFraud']
del X['isFraud']

# Eliminate columns shown to be irrelevant for analysis in the EDA
X = X.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)

# Binary-encoding of labelled data in 'type'
X.loc[X.type == 'TRANSFER', 'type'] = 0           
X.loc[X.type == 'CASH_OUT', 'type'] = 1
X.type = X.type.astype(int) # convert dtype('O') to dtype(int)

X.head()

Y.head()

Xfraud = X.loc[Y == 1]
XnonFraud = X.loc[Y == 0]

X['errorBalanceOrig'] = X.newBalanceOrig + X.amount - X.oldBalanceOrig
X['errorBalanceDest'] = X.oldBalanceDest + X.amount - X.newBalanceDest

limit = len(X)

def plotStrip(x, y, hue, figsize = (14, 9)):
    
    fig = plt.figure(figsize = figsize)
    colours = plt.cm.tab10(np.linspace(0, 1, 9))
    with sns.axes_style('ticks'):
        ax = sns.stripplot(x, y, \
             hue = hue, jitter = 0.4, marker = '.', \
             size = 4, palette = colours)
        ax.set_xlabel('')
        ax.set_xticklabels(['genuine', 'fraudulent'], size = 16)
        for axis in ['top','bottom','left','right']:
            ax.spines[axis].set_linewidth(2)

        handles, labels = ax.get_legend_handles_labels()
        plt.legend(handles, ['Transfer', 'Cash out'], bbox_to_anchor=(1, 1), \
               loc=2, borderaxespad=0, fontsize = 16)
    return [fig,ax]


#fig = plt.figure(figsize=(14,9))
fig, ax = plotStrip(Y[:limit], X.step[:limit], X.type[:limit])
ax.set_ylabel('Time [hour]', size = 16)
ax.set_title('Striped vs. homogenous fingerprints of genuine and fraudulent \ transactions over time', size = 20)
st.pyplot(fig)

st.write("The plot above shows how the fraudulent and genuine transactions yield different fingerprints when their \
    dispersion is viewed over time. It is clear that fraudulent transactions are more homogeneously distributed \
    over time compared to genuine transactions. Also apparent is that CASH-OUTs outnumber TRANSFERs in genuine \
    transactions, in contrast to a balanced distribution between them in fraudulent transactions.")
st.text("")


limit = len(X)
#fig = plt.figure(figsize=(14,9))
fig, ax = plotStrip(Y[:limit], X.amount[:limit], X.type[:limit], figsize = (14, 9))
ax.set_ylabel('Amount', size = 16)
ax.set_title('Same-signed fingerprints of genuine \ and fraudulent transactions over amount', size = 18)
st.pyplot(fig)

#########################################################



#########################################################           Selecting Attributes
st.markdown("""---

### **iii) Atrributes to select for model**
""")
st.text("")

fig, ax = plt.subplots(figsize=(6,4))
ax = sns.heatmap(tmp.corr(), linecolor='black', linewidth=1, cmap='Blues')
st.pyplot(fig)

Xfraud = X.loc[Y == 1]            # update Xfraud & XnonFraud with cleaned data
XnonFraud = X.loc[Y == 0]
                  
correlationNonFraud = XnonFraud.loc[:, X.columns != 'step'].corr()
mask = np.zeros_like(correlationNonFraud)
indices = np.triu_indices_from(correlationNonFraud)
mask[indices] = True



#Matrix Size
grid_kws = {"width_ratios": (.9, .9, .05), "wspace": 0.2}
f, (ax1, ax2, cbar_ax) = plt.subplots(1, 3, gridspec_kw=grid_kws, figsize = (14, 9))

#Genuine Transactions
cmap = sns.diverging_palette(220, 8, as_cmap=True)
ax1 =sns.heatmap(correlationNonFraud, ax = ax1, vmin = -1, vmax = 1, \
    cmap = cmap, square = False, linewidths = 0.5, mask = mask, cbar = False)
ax1.set_xticklabels(ax1.get_xticklabels(), size = 16); 
ax1.set_yticklabels(ax1.get_yticklabels(), size = 16); 
ax1.set_title('Genuine \n transactions', size = 20)

#Fradulent Transactions
correlationFraud = Xfraud.loc[:, X.columns != 'step'].corr()
ax2 = sns.heatmap(correlationFraud, vmin = -1, vmax = 1, cmap = cmap, \
 ax = ax2, square = False, linewidths = 0.5, mask = mask, yticklabels = False, \
    cbar_ax = cbar_ax, cbar_kws={'orientation': 'vertical', \
                                 'ticks': [-1, -0.5, 0, 0.5, 1]})
ax2.set_xticklabels(ax2.get_xticklabels(), size = 16); 
ax2.set_title('Fraudulent \n transactions', size = 20)

cbar_ax.set_yticklabels(cbar_ax.get_yticklabels(), size = 14)

st.pyplot(f)


df_fraud = df.loc[(df.isFraud == 1)]
df_leg = df.loc[(df.isFraud == 1)]
corr_fraud = df_fraud.loc[:, df_fraud.columns != 'step'].corr(method="spearman")

def top_corr(corr,target, n):
  abs_corr = (corr.abs().unstack())[target].to_dict()
  abs_corr.pop(target)
  sorted_corr = dict( sorted(abs_corr.items(), key=lambda item: item[1], reverse=True))
  if n>len(sorted_corr): 
    n = sorted_corr
  sorted_corr = [item for item in sorted_corr.keys()][:n]
  return sorted_corr

temp = top_corr(corr_fraud,"isFraud",5)
st.write("The top %d factors which affect the target are: "%(len(temp)))
for i in range(len(temp)):
  st.write("%d. %s"%(i+1,temp[i]))
#########################################################





#############################################################   Viewing Skewness of data
st.markdown("""---

### **iv) Skewness of Data and Attributes**
""")
st.text("")
st.write("If the data we work on is skewed, the model is more likely to successfully predict the skewed class and suffers \
    when predicting other classes. In more extreme cases, it may violate the model assumptions (e.g. Logistic Regression)\
    or impair the interpretation of feature importance. We tested 2 methods to correct the skewness: ")
st.markdown("> * **Square Root Method:** We take the square root of the feature to reduce the skewness. This only has a \
    moderate effect compared to logarithms but can be applied to zero values.")
st.markdown("> * **Box Cox Method:** This method transform non-normal dependent variables in our data to a normal shape. \
    It uses logarithmic tranformation to more effecively handle skewness.")
st.text("")
st.markdown("""
    **a) Transaction Amount**
    """)
st.text("")

tmp['amount_boxcox'] = preprocessing.scale(boxcox(tmp['amount']+1)[0])

figure = plt.figure(figsize=(16, 5))
figure.add_subplot(131) 
plt.hist(tmp['amount'] ,facecolor='blue',alpha=0.75) 
plt.xlabel("Transaction amount") 
plt.title("Transaction amount ") 
plt.text(10,100000,"Skewness: {0:.2f}".format(skew(tmp['amount'])))

figure.add_subplot(132)
plt.hist(np.sqrt(tmp['amount']), facecolor = 'red', alpha=0.5)
plt.xlabel("Square root of amount")
plt.title("Using SQRT on amount")
plt.text(10, 100000, "Skewness: {0:.2f}".format(skew(np.sqrt(tmp['amount']))))

figure.add_subplot(133)
plt.hist(tmp['amount_boxcox'], facecolor = 'red', alpha=0.5)
plt.xlabel("Box cox of amount")
plt.title("Using Box cox on amount")
plt.text(10, 100000, "Skewness: {0:.2f}".format(skew(tmp['amount_boxcox'])))

st.pyplot(figure)


######################################### Old balance

st.text("")
st.markdown("""
    **b. Old Balance of Account**
    """)
st.text("")

tmp['oldbalanceOrg_boxcox'] = preprocessing.scale(boxcox(tmp['oldBalanceOrig']+1)[0])

figure = plt.figure(figsize=(16, 5))
figure.add_subplot(131) 
plt.hist(tmp['oldBalanceOrig'] ,facecolor='blue',alpha=0.75) 
plt.xlabel("Old balance originated") 
plt.title("Old balance original") 
plt.text(2,100000,"Skewness: {0:.2f}".format(skew(tmp['oldBalanceOrig'])))


figure.add_subplot(132)
plt.hist(np.sqrt(tmp['oldBalanceOrig']), facecolor = 'red', alpha=0.5)
plt.xlabel("Square root of oldBal")
plt.title("SQRT on oldbalanceOrg")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(np.sqrt(tmp['oldBalanceOrig']))))

figure.add_subplot(133)
plt.hist(tmp['oldbalanceOrg_boxcox'], facecolor = 'red', alpha=0.5)
plt.xlabel("Box cox of oldBal")
plt.title("Box cox on oldbalanceOrg")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(tmp['oldbalanceOrg_boxcox'])))

st.pyplot(figure)


######################################### New Balance

st.text("")
st.markdown("""
    **c. New Balance of account**
    """)
st.text("")

tmp['newbalanceOrg_boxcox'] = preprocessing.scale(boxcox(tmp['newBalanceOrig']+1)[0])

figure = plt.figure(figsize=(16, 5))
figure.add_subplot(131) 
plt.hist(tmp['newBalanceOrig'] ,facecolor='blue',alpha=0.75) 
plt.xlabel("New balance originated") 
plt.title("New balance org") 
plt.text(2,100000,"Skewness: {0:.2f}".format(skew(tmp['newBalanceOrig'])))


figure.add_subplot(132)
plt.hist(np.sqrt(tmp['newBalanceOrig']), facecolor = 'red', alpha=0.5)
plt.xlabel("Square root of newBal")
plt.title("SQRT on newbalanceOrig")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(np.sqrt(tmp['newBalanceOrig']))))

figure.add_subplot(133)
plt.hist(tmp['newbalanceOrg_boxcox'], facecolor = 'red', alpha=0.5)
plt.xlabel("Box cox of newBal")
plt.title("Box cox on newbalanceOrig")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(tmp['newbalanceOrg_boxcox'])))

st.pyplot(figure)


######################################### Old balance of Destination

st.text("")
st.markdown("""
    **d. Old Balance of destination**
    """)
st.text("")

tmp['oldbalanceDest_boxcox'] = preprocessing.scale(boxcox(tmp['oldBalanceDest']+1)[0])

figure = plt.figure(figsize=(16, 5))
figure.add_subplot(131) 
plt.hist(tmp['oldBalanceDest'] ,facecolor='blue',alpha=0.75) 
plt.xlabel("Old balance desinated") 
plt.title("Old balance dest") 
plt.text(2,100000,"Skewness: {0:.2f}".format(skew(tmp['oldBalanceDest'])))


figure.add_subplot(132)
plt.hist(np.sqrt(tmp['oldBalanceDest']), facecolor = 'red', alpha=0.5)
plt.xlabel("Square root of oldBalDest")
plt.title("SQRT on oldbalanceDest")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(np.sqrt(tmp['oldBalanceDest']))))

figure.add_subplot(133)
plt.hist(tmp['oldbalanceDest_boxcox'], facecolor = 'red', alpha=0.5)
plt.xlabel("Box cox of oldbalanceDest")
plt.title("Box cox on oldbalanceDest")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(tmp['oldbalanceDest_boxcox'])))

st.pyplot(figure)


######################################### New balance of Destination

st.text("")
st.markdown("""
    **e. New Balance of Destination**
    """)
st.text("")

tmp['newbalanceDest_boxcox'] = preprocessing.scale(boxcox(tmp['newBalanceDest']+1)[0])

figure = plt.figure(figsize=(16, 5))
figure.add_subplot(131) 
plt.hist(tmp['newBalanceDest'] ,facecolor='blue',alpha=0.75) 
plt.xlabel("newbalanceDest") 
plt.title("newbalanceDest") 
plt.text(2,100000,"Skewness: {0:.2f}".format(skew(tmp['newBalanceDest'])))


figure.add_subplot(132)
plt.hist(np.sqrt(tmp['newBalanceDest']), facecolor = 'red', alpha=0.5)
plt.xlabel("Square root of newbalanceDest")
plt.title("SQRT on newbalanceDest")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(np.sqrt(tmp['newBalanceDest']))))

figure.add_subplot(133)
plt.hist(tmp['newbalanceDest_boxcox'], facecolor = 'red', alpha=0.5)
plt.xlabel("Box cox of newbalanceDest")
plt.title("Box cox on newbalanceDest")
plt.text(2, 100000, "Skewness: {0:.2f}".format(skew(tmp['newbalanceDest_boxcox'])))

st.pyplot(figure)



#########################################################  Model Results Function

def model_result(clf,x_test,y_test):
    y_prob=clf.predict_proba(x_test)
    y_pred=clf.predict(x_test)
    auprc = round((average_precision_score(y_test, y_prob[:, 1])),3)
    f1 = round(f1_score(y_test,y_pred),3)
    acc = round(accuracy_score(y_test,y_pred),3)
    recall = round(recall_score(y_test, y_pred, average='micro'),3)

    st.text("")
    st.write('AUPRC :', auprc)
    st.write('F1 - score :', f1)
    st.write("Accuracy Score: ", acc)
    st.write("Recall Score: ", recall)
    st.text("")
    st.text("")
    st.markdown("**Classification Report:**")
    st.text(classification_report(y_test,y_pred))
    st.text("")

    cm = confusion_matrix(y_test, y_pred)
    fig = plt.figure(figsize = (10,7))
    ax = sns.heatmap(cm, annot=True, annot_kws={"size": 16},
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'],
            linewidth=1,
            linecolor='black',
            cmap ="Blues"
            )
    plt.title("Confusion matrix",fontsize=16)
    st.pyplot(fig)  
    return {'auprc': auprc, 'f1':f1, 'accuracy':acc, 'recall':recall}


def cnn_result(model, X_test, y_test):

    yhat_classes = model.predict_classes(X_test, verbose=0)
    yhat_classes = yhat_classes[:, 0]
    yhat_probs = model.predict(X_test, verbose=0)
    yhat_probs = yhat_probs[:, 0]
    
    acc = round(accuracy_score(y_test, yhat_classes),3)
    recall = round(recall_score(y_test, yhat_classes),3)
    f1 = round(f1_score(y_test, yhat_classes),3)
    auprc = round((average_precision_score(y_test, yhat_probs)),3)

    st.text("")
    st.write('AUPRC :', auprc)
    st.write('F1 - score :', f1)
    st.write("Accuracy Score: ", acc)
    st.write("Recall Score: ", recall)
    st.text("")
    st.text("")
    st.markdown("**Classification Report:**")
    st.text(classification_report(y_test,yhat_classes))
    st.text("")

    cm = confusion_matrix(y_test, yhat_classes)
    fig = plt.figure(figsize = (10,7))
    ax = sns.heatmap(cm, annot=True, annot_kws={"size": 16},
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'],
            linewidth=1,
            linecolor='black',
            cmap ="Blues"
            )
    plt.title("Confusion matrix",fontsize=16)
    st.pyplot(fig) 
    return {'auprc': auprc, 'f1':f1, 'accuracy':acc, 'recall':recall}








##########################################################################  Models
st.markdown("""---

# **D. Models**
""")
st.text("")

st.markdown("""
We will be implementing 5 different methods to compare the results:
""")
st.markdown(" > 1. Logistic Regression (Under sampling) ")
st.markdown(" > 2. Logistic Regression (SMOTE) ")
st.markdown(" > 3. Neural Networks  (Under sampling)")
st.markdown(" > 4. Neural Networks (SMOTE) ")
st.markdown(" > 5. XGBoost ")
st.markdown(" > 6. CATBoost ")
st.text("")

data_counts = Counter(Y)
st.write("Legitimate Transactions: {:,}".format(data_counts[0]))
st.write("Fraudulent Transactions: {:,}".format(data_counts[1]))





################################################################### 1. Logistic Regression  
st.text("")
st.text("") 
st.markdown("""---

## **1. Logistic Regression**
""")
st.text("")


################################################ A. Under Sampling
st.text("")
st.markdown("""
### **A. Under Sampling Data**
""")
st.text("")

rus = RandomUnderSampler(random_state=0)
rus.fit(X, Y)
X_under, Y_under = rus.fit_resample(X, Y)

data_under = Counter(Y_under)
st.markdown(" > * Legitimate Transactions: {:,}".format(data_under[0]))
st.markdown(" > * Fraudulent Transactions: {:,}".format(data_under[1]))
st.text("")


X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, Y_under, test_size = 0.25, random_state = 42)

st.write(" > **Number of transactions for training:** ", format(len(X_train_under),',d'))
st.write(" > **Number of transactions for testing:** ", format(len(X_test_under), ',d'))
st.write(" > **Total number of transactions:** ", format(len(X_train_under)+len(X_test_under), ',d'))
st.text("")
st.text("")


if not os.path.isfile("./models/LR_under.sav"):
    start = time.time()
    lr_under = LogisticRegression()
    lr_under.fit(X_train_under, y_train_under)
    end = time.time()
    time_lr = round(end-start,3)
    pickle.dump(lr_under, open('./models/LR_under.sav', 'wb'))
    pickle.dump(time_lr, open('./times/LR_under.sav', 'wb'))
else:
    lr_under = pickle.load(open('./models/LR_under.sav', 'rb'))
    time_lr = pickle.load(open('./times/LR_under.sav', 'rb'))


y_pred_under = lr_under.predict(X_test_under)

st.write(" > **Time taken:** ", round(time_lr,3), "seconds")

st.text("")
st.text("")
st.markdown(""" **Test Results:** """)
lr_under_results = model_result(lr_under,X_test_under,y_test_under)



##################################################### B. Over Sampling
st.text("")
st.text("")
st.markdown("""
### **B. Oversampling (SMOTE)**
 """)
st.text("")


from imblearn.over_sampling import SMOTE

oversample = SMOTE()
X_over, Y_over = oversample.fit_resample(X, Y)

data_over = Counter(Y_over)
st.write(" > * Legitimate: {:,}".format(data_over[0]))
st.write("> * Fraudulent: {:,}".format(data_over[1]))
st.text("")

X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over,Y_over, test_size = 0.25, random_state = 42)
st.write(" > **Number transactions train dataset:** ", format(len(X_train_over),',d'))
st.write(" > **Number transactions test dataset:** ", format(len(X_test_over), ',d'))
st.write(" > **Total number of transactions:** ", format(len(X_train_over)+len(X_test_over), ',d'))
st.text("")
st.text("")


if not os.path.isfile("./models/LR_over.sav"):
    start = time.time()
    lr_over = LogisticRegression()
    lr_over.fit(X_train_over, y_train_over)
    end = time.time()
    time_lr_smote = round(end-start,3)
    pickle.dump(lr_over, open('./models/LR_over.sav', 'wb'))
    pickle.dump(time_lr_smote, open('./times/LR_over.sav', 'wb'))
else:
    lr_over = pickle.load(open('./models/LR_over.sav', 'rb'))
    time_lr_smote = pickle.load(open('./times/LR_over.sav', 'rb'))


y_pred_over = lr_over.predict(X_test_over)

st.write(" > **Time taken:** ", round(time_lr_smote,3), "seconds")

st.text("")
st.text("")
st.markdown(""" **Test Results:** """)
lr_over_results = model_result(lr_over,X_test_over,y_test_over)

####################################################################################






################################################################################### 2. CNN
st.text("")
st.markdown(""" ---

## **2. Convolutional Neural Network**
""")
st.text("")

######################################### A. Under Sampling
st.text("")
st.markdown("""
### **A. Under Sampling Data**
""")
st.text("")

rus = RandomUnderSampler(random_state=0)
rus.fit(X, Y)
X_under, Y_under = rus.fit_resample(X, Y)

data_under = Counter(Y_under)
st.markdown(" > * Legitimate Transactions: {:,}".format(data_under[0]))
st.markdown(" > * Fraudulent Transactions: {:,}".format(data_under[1]))
st.text("")

train_ratio = 0.75
validation_ratio = 0.15
test_ratio = 0.10

X_train_under_cnn, X_test_under_cnn, y_train_under_cnn, y_test_under_cnn = \
    train_test_split(X_under, Y_under, test_size = 1 - train_ratio)
X_val_under_cnn, X_test_under_cnn, y_val_under_cnn, y_test_under_cnn = \
    train_test_split(X_test_under, y_test_under, test_size = test_ratio/(test_ratio + validation_ratio)) 

st.write(" > **Number of transactions for training:** ", format(len(X_train_under_cnn),',d'))
st.write(" > **Number of transactions for validation:** ", format(len(X_val_under_cnn),',d'))
st.write(" > **Number of transactions for testing:** ", format(len(X_test_under_cnn), ',d'))
st.write(" > **Total number of transactions:** ", format(len(X_train_under_cnn)+len(X_val_under_cnn)+len(X_test_under_cnn), ',d'))
st.text("")
st.text("")


if not os.path.isfile("./models/CNN_under.sav"):
    model = keras.Sequential([
        keras.layers.Dense(512, activation='relu', input_shape=(X_train_under_cnn.shape[-1],)),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(256, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid'),
    ])
    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')
    start = time.time()
    history= model.fit(X_train_under_cnn, y_train_under_cnn, 
                    validation_data=(X_val_under_cnn, y_val_under_cnn),
                    batch_size=64, 
                    epochs=10
                    )
    end = time.time()
    time_cnn_under = round(end-start,3)
    model.save("./models/NN_under")
    pickle.dump(time_cnn_under, open('./times/NN_under.sav', 'wb'))
else:
    model = keras.models.load_model('./models/NN_under')
    time_cnn_under = pickle.load(open('./times/NN_under.sav', 'rb'))


y_pred_under_cnn = model.predict(X_test_under_cnn)

st.write(" > **Time taken:** ", round(time_lr,3), "seconds")

st.text("")
st.text("")
st.markdown(""" **Test Results:** """)
cnn_under_results = cnn_result(model,X_test_under,y_test_under)
st.text("")



######################################### B. Over Sampling
st.text("")
st.text("")
st.markdown("""
### **B. Oversampling (SMOTE)**
 """)
st.text("")


from imblearn.over_sampling import SMOTE

oversample = SMOTE()
X_over, Y_over = oversample.fit_resample(X, Y)

data_over = Counter(Y_over)
st.write(" > * Legitimate: {:,}".format(data_over[0]))
st.write("> * Fraudulent: {:,}".format(data_over[1]))
st.text("")

X_train_over_cnn, X_test_over_cnn, y_train_over_cnn, y_test_over_cnn = \
    train_test_split(X_over, Y_over, test_size = 1 - train_ratio)
X_val_over_cnn, X_test_over_cnn, y_val_over_cnn, y_test_over_cnn = \
    train_test_split(X_test_over, y_test_over, test_size = test_ratio/(test_ratio + validation_ratio)) 

st.write(" > **Number of transactions for training:** ", format(len(X_train_over_cnn),',d'))
st.write(" > **Number of transactions for validation:** ", format(len(X_val_over_cnn),',d'))
st.write(" > **Number of transactions for testing:** ", format(len(X_test_over_cnn), ',d'))
st.write(" > **Total number of transactions:** ", format(len(X_train_over_cnn)+len(X_val_over_cnn)+len(X_test_over_cnn), ',d'))
st.text("")
st.text("")


if not os.path.isfile("./models/NN_over.sav"):
    model = keras.Sequential([
        keras.layers.Dense(512, activation='relu', input_shape=(X_train_over_cnn.shape[-1],)),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(256, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid'),
    ])
    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')
    start = time.time()
    history= model.fit(X_train_over_cnn, y_train_over_cnn, 
                    validation_data=(X_val_over_cnn, y_val_over_cnn),
                    batch_size=2048, 
                    epochs=10
                    )
    end = time.time()
    time_cnn_over = round(end-start,3)
    model.save("./models/NN_over")
    pickle.dump(time_cnn_under, open('./times/NN_over.sav', 'wb'))

else:
    model = keras.models.load_model('./models/NN_over')
    time_cnn_over = pickle.load(open('./times/NN_over.sav', 'rb'))

y_pred_over_cnn = model.predict(X_test_over_cnn)

st.write(" > **Time taken:** ", round(time_lr_smote,3), "seconds")

st.text("")
st.text("")
st.markdown(""" **Test Results:** """)
cnn_over_results = cnn_result(model,X_test_over,y_test_over)
st.text("")

############################################################################




################################################################################ 3.XGBoost
st.markdown(""" ---

## **3. XGBoost**
""")
st.text("")


X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X, Y, test_size = 0.25, random_state = 0)
st.write(" > **Number transactions train dataset:** ", format(len(X_train_xgb),',d'))
st.write(" > **Number transactions test dataset:** ", format(len(X_test_xgb), ',d'))
st.write(" > **Total number of transactions:** ", format(len(X_train_xgb)+len(X_test_xgb), ',d'))
st.text("")
st.text("")

if not os.path.isfile("./models/XGB.sav"):
    start = time.time()
    xgb = XGBClassifier()
    xgb.fit(X_train_xgb, y_train_xgb)
    end = time.time()
    time_xgb = round(end-start,3)
    pickle.dump(xgb, open('./models/XGB.sav', 'wb'))
    pickle.dump(time_xgb, open('./times/XGB.sav', 'wb'))
else:
    xgb = pickle.load(open('./models/XGB.sav', 'rb'))
    time_xgb = pickle.load(open('./times/XGB.sav', 'rb'))


y_pred_xgb = xgb.predict(X_test_xgb)

st.write("> **Time taken:** ", round(time_xgb,3), "seconds")
st.text("")

fig = plt.figure(figsize = (12, 8))
ax = fig.add_subplot(111)
colours = plt.cm.Set1(np.linspace(0, 1, 9))

ax = plot_importance(xgb, height = 1, color = colours, grid = False, \
                     show_values = False, importance_type = 'cover', ax = ax)
for axis in ['top','bottom','left','right']:
            ax.spines[axis].set_linewidth(2)
        
ax.set_xlabel('Importance score', size = 16)
ax.set_ylabel('Features', size = 16)
ax.set_yticklabels(ax.get_yticklabels(), size = 12)
ax.set_title('Ordering of features by importance to the model learnt', size = 20)
st.pyplot(fig)
st.text("")

image_xgb = xgboost.to_graphviz(xgb)
image_xgb.graph_attr = {'dpi':'600'}
image_xgb.render('./models/xgboost_tree', format = 'png')
fig = plt.figure(figsize=(20,10))
img_xgb = mpimg.imread('./models/xgboost_tree.png')
imgplot = plt.imshow(img_xgb) 
st.pyplot(fig)

st.text("")

print ('Test Results: ')
xgb_results = model_result(xgb,X_test_xgb,y_test_xgb)

# Commented out IPython magic to ensure Python compatibility.
# %time


st.text("")
weights = (Y == 0).sum() / (1.0 * (Y == 1).sum())
                              
if not os.path.isfile("./models/trainSizes.sav"):
    trainSizes, trainScores, crossValScores = learning_curve(XGBClassifier(n_jobs = 4, max_depth = 3,scale_pos_weight = weights), 
                                                         X_train_xgb, y_train_xgb, scoring = 'average_precision')
    pickle.dump(trainSizes, open('./models/trainSizes.sav', 'wb'))
    pickle.dump(trainScores, open('./models/trainScores.sav', 'wb'))
    pickle.dump(crossValScores, open('./models/crossValScores.sav', 'wb'))
else:
    xgb = pickle.load(open('./models/XGB.sav', 'rb'))
    trainSizes = pickle.load(open('./models/trainSizes.sav', 'rb'))
    trainScores = pickle.load(open('./models/trainScores.sav', 'rb'))
    crossValScores = pickle.load(open('./models/crossValScores.sav', 'rb'))



trainScoresMean = np.mean(trainScores, axis=1)
trainScoresStd = np.std(trainScores, axis=1)
crossValScoresMean = np.mean(crossValScores, axis=1)
crossValScoresStd = np.std(crossValScores, axis=1)

colours = plt.cm.tab10(np.linspace(0, 1, 9))

fig = plt.figure(figsize = (14, 9))
plt.fill_between(trainSizes, trainScoresMean - trainScoresStd, trainScoresMean + trainScoresStd, alpha=0.1, color=colours[0])
plt.fill_between(trainSizes, crossValScoresMean - crossValScoresStd, crossValScoresMean + crossValScoresStd, alpha=0.1, color=colours[1])
plt.plot(trainSizes, trainScores.mean(axis = 1), 'o-', label = 'train', color = colours[0])
plt.plot(trainSizes, crossValScores.mean(axis = 1), 'o-', label = 'cross-val', color = colours[1])

ax = plt.gca()
for axis in ['top','bottom','left','right']:
    ax.spines[axis].set_linewidth(2)

handles, labels = ax.get_legend_handles_labels()
plt.legend(handles, ['train', 'cross-val'], bbox_to_anchor=(0.8, 0.15), loc=2, borderaxespad=0, fontsize = 16)
plt.xlabel('Training set size', size = 16); 
plt.ylabel('AUPRC', size = 16)
plt.title('Learning curves indicate slightly underfit model', size = 20)

st.pyplot(fig)
st.text("")

###########################################################################################




################################################################################### 4. CATBoost
st.markdown(""" ---

## **4. CATBoost**
""")
st.text("")

X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X, Y, test_size = 0.25,random_state=0)
st.write(" > **Number transactions train dataset:** ", format(len(X_train_cat),',d'))
st.write(" > **Number transactions test dataset:** ", format(len(X_test_cat), ',d'))
st.write(" > **Total number of transactions:** ", format(len(X_train_cat)+len(X_test_cat), ',d'))
st.text("")
st.text("")


if not os.path.isfile("./models/CAT.sav"):
    start = time.time()
    model_cat = CatBoostClassifier(iterations=20, learning_rate=1, depth=4)
    model_cat.fit(X_train_cat,y_train_cat)
    end = time.time()
    time_cat = round(end-start,3)
    pickle.dump(model_cat, open('./models/CAT.sav', 'wb'))
    pickle.dump(time_cat, open('./times/CAT.sav', 'wb'))
else:
    model_cat = pickle.load(open('./models/CAT.sav', 'rb'))
    time_cat = pickle.load(open('./times/CAT.sav', 'rb'))


y_pred_cat = model_cat.predict(X_test_cat)

st.write(" > **Time taken:** ", round(time_cat,3), "seconds")
st.text("")
st.text("")

def plot_feature_importance(importance,names,model_type):
    
    #Create arrays from feature importance and feature names
    feature_importance = np.array(importance)
    feature_names = np.array(names)
    
    #Create a DataFrame using a Dictionary
    data={'feature_names':feature_names,'feature_importance':feature_importance}
    fi_df = pd.DataFrame(data)
    
    #Sort the DataFrame in order decreasing feature importance
    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    
    #Define size of bar plot
    fig = plt.figure(figsize=(10,8))
    #Plot Searborn bar chart
    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
    #Add chart labels
    plt.title(model_type + ' FEATURE IMPORTANCE')
    plt.xlabel('FEATURE IMPORTANCE')
    plt.ylabel('FEATURE NAMES')
    return fig

fig = plot_feature_importance(model_cat.get_feature_importance(), X_train_cat.columns,'CATBOOST')
st.pyplot(fig)
st.text("")
st.text("")

image_cat = model_cat.plot_tree(tree_idx=0)
image_cat.graph_attr = {'dpi':'600'}
image_cat.render('./models/catboost_tree', format = 'png')

fig = plt.figure(figsize=(20,10))
img_cat = mpimg.imread('./models/catboost_tree.png')
imgplot = plt.imshow(img_cat) 
st.pyplot(fig)

st.markdown(""" **Test Results:** """)
cat_results = model_result(model_cat,X_test_cat,y_test_cat)
st.text("")

##############################################################################



######################################################################### Results



st.markdown("""---

## **E. Results**
""")
st.text("")


label_cols = ["Model", "Accuracy", "Recall", "F-Score", "AUPRC", "Time Taken (Seconds)"]
label_rows = ["Logistic Regression (Under)","Logistic Regression (Over)", "CNN (Under)","CNN (Over)", "XGBoost", "CatBoost"] 

acc = [lr_under_results["accuracy"], lr_over_results["accuracy"],cnn_under_results["accuracy"], \
    cnn_over_results["accuracy"], xgb_results["accuracy"], cat_results["accuracy"]]

recall = [lr_under_results["recall"], lr_over_results["recall"], cnn_under_results["recall"], \
    cnn_over_results["recall"], xgb_results["recall"], cat_results["recall"]]

auprc = [lr_under_results["auprc"], lr_over_results["auprc"], cnn_under_results["auprc"], \
    cnn_over_results["auprc"], xgb_results["auprc"], cat_results["auprc"]]

f_scores = [lr_under_results["f1"], lr_over_results["f1"], cnn_under_results["f1"], \
    cnn_over_results["f1"], xgb_results["f1"], cat_results["f1"]]

times = [time_lr, time_lr_smote, time_cnn_under, time_cnn_over, time_xgb, time_cat]

results = pd.DataFrame({"Model":label_rows, "Accuracy": acc, "Recall":recall, "F-Scores":f_scores, "AUPRC":auprc, "Time Taken":times})
results.columns = label_cols
st.dataframe(results.head(6))
st.text("")

